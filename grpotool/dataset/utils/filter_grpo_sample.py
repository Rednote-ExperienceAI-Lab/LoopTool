# å°†åœ¨è®­ç»ƒæ•°æ®ä¸Šæ¨ç†é”™è¯¯çš„æ ·æœ¬å†æ”¶é›†èµ·æ¥ï¼Œé‡æ–°ä½œä¸ºä¸‹ä¸€è½®æ ·æœ¬çš„ç”Ÿæˆ, ç”Ÿæˆä½¿ç”¨çš„æ˜¯å®˜æ–¹ç»™çš„è„šæœ¬ï¼Œæœ¬è„šæœ¬æ•´ç†ç”Ÿæˆçš„ç»“æœï¼Œå¹¶ä¸åŸç»“æœè¿›è¡Œå¯¹æ¯”ã€‚
import re
import os
import json
import numpy as np
import pandas as pd
import argparse
import ast
import builtins
import copy
import operator
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, List, Optional, Type, Union, Dict, Tuple, Any
from collections import Counter
from openai import OpenAI


class ClientManager:
    """ç®¡ç†å¤šä¸ªOpenAIå®¢æˆ·ç«¯"""
    def __init__(self, api_keys: List[str], base_url: str):
        self.clients = []
        for api_key in api_keys:
            client = OpenAI(base_url=base_url, api_key=api_key)
            self.clients.append(client)
        self.lock = threading.Lock()
    
    def get_random_client(self) -> OpenAI:
        """éšæœºè·å–ä¸€ä¸ªå®¢æˆ·ç«¯"""
        with self.lock:
            return random.choice(self.clients)

def resolve_ast_by_type(value):
    if isinstance(value, ast.Constant):
        if value.value is Ellipsis:
            output = "..."
        else:
            output = value.value
    elif isinstance(value, ast.UnaryOp):
        output = -value.operand.value
    elif isinstance(value, ast.List):
        output = [resolve_ast_by_type(v) for v in value.elts]
    elif isinstance(value, ast.Dict):
        output = {
            resolve_ast_by_type(k): resolve_ast_by_type(v)
            for k, v in zip(value.keys, value.values)
        }
    elif isinstance(
        value, ast.NameConstant
    ):  # Added this condition to handle boolean values
        output = value.value
    elif isinstance(
        value, ast.BinOp
    ):  # Added this condition to handle function calls as arguments
        output = eval(ast.unparse(value))
    elif isinstance(value, ast.Name):
        output = value.id
    elif isinstance(value, ast.Call):
        if len(value.keywords) == 0:
            output = ast.unparse(value)
        else:
            output = resolve_ast_call(value)
    elif isinstance(value, ast.Tuple):
        output = tuple(resolve_ast_by_type(v) for v in value.elts)
    elif isinstance(value, ast.Lambda):
        output = eval(ast.unparse(value.body[0].value))
    elif isinstance(value, ast.Ellipsis):
        output = "..."
    elif isinstance(value, ast.Subscript):
        try:
            output = ast.unparse(value.body[0].value)
        except:
            output = ast.unparse(value.value) + "[" + ast.unparse(value.slice) + "]"
    else:
        raise Exception(f"Unsupported AST type: {type(value)}")
    return output

def resolve_ast_call(elem):
    # Handle nested attributes for deeply nested module paths
    func_parts = []
    func_part = elem.func
    while isinstance(func_part, ast.Attribute):
        func_parts.append(func_part.attr)
        func_part = func_part.value
    if isinstance(func_part, ast.Name):
        func_parts.append(func_part.id)
    func_name = ".".join(reversed(func_parts))
    args_dict = {}
    for arg in elem.keywords:
        output = resolve_ast_by_type(arg.value)
        args_dict[arg.arg] = output
    return {func_name: args_dict}

def ast_parse(input_str: str, language: str="Python") -> list[dict]:
    try:
        cleaned_input = input_str.strip("[]'")
        parsed = ast.parse(cleaned_input, mode="eval")
        extracted = []
        if isinstance(parsed.body, ast.Call):
            extracted.append(resolve_ast_call(parsed.body))
        else:
            for elem in parsed.body.elts:
                assert isinstance(elem, ast.Call)
                extracted.append(resolve_ast_call(elem))
        return extracted
    except Exception as e:
        # print("è§£æå‡ºç°å¼‚å¸¸")
        return []

def default_decode_execute_prompting(result: str):
    result = result.strip("`\n ")
    if not result.startswith("["):
        result = "[" + result
    if not result.endswith("]"):
        result = result + "]"
    return ast_parse(result)

def compare_parsed_content(parsed1, parsed2):
    """
    æ¯”è¾ƒä¸¤ä¸ªè§£æåçš„å†…å®¹æ˜¯å¦ä¸€è‡´ï¼Œå¿½ç•¥åˆ—è¡¨ä¸­å…ƒç´ çš„é¡ºåºä»¥åŠå­—å…¸ä¸­é”®çš„é¡ºåºã€‚
    
    å‚æ•°:
    parsed1 (list of dict): ç¬¬ä¸€ä¸ªè§£æåçš„å†…å®¹
    parsed2 (list of dict): ç¬¬äºŒä¸ªè§£æåçš„å†…å®¹
    
    è¿”å›:
    bool: å¦‚æœä¸¤ä¸ªè§£æåçš„å†…å®¹ä¸€è‡´ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› False
    """
    if len(parsed1) != len(parsed2):
        return False
        
    def convert_to_hashable(data):
        """
        å°†å­—å…¸è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„ frozensetï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒã€‚
        """
        if isinstance(data, dict):
            return frozenset((key, convert_to_hashable(value)) for key, value in data.items())
        elif isinstance(data, list):
            return frozenset(convert_to_hashable(item) for item in data)
        else:
            return data

    # å°†æ¯ä¸ªå­—å…¸è½¬æ¢ä¸º frozensetï¼Œå¹¶å¯¹åˆ—è¡¨è¿›è¡Œ Counter è®¡æ•°
    counter1 = Counter(convert_to_hashable(parsed1))
    counter2 = Counter(convert_to_hashable(parsed2))

    # æ¯”è¾ƒä¸¤ä¸ª Counter æ˜¯å¦ç›¸ç­‰
    return counter1 == counter2

def load_and_show_content(original_path: str, inference_path: str):
    """åŠ è½½å¹¶å±•ç¤ºoutputå’Œresponseå†…å®¹"""
    # åŠ è½½æ•°æ®
    original_df = pd.read_parquet(original_path)
    inference_df = pd.read_parquet(inference_path)
    
    print(f"Original dataset shape: {original_df.shape}")
    print(f"Inference dataset shape: {inference_df.shape}")
    print(f"Original columns: {list(original_df.columns)}")
    print(f"Inference columns: {list(inference_df.columns)}")
    
    # å±•ç¤ºå‡ ä¸ªæ ·æœ¬çš„å†…å®¹
    n_samples = 10
    
    for i in range(min(n_samples, len(original_df))):
        print(f"\n{'='*60}")
        print(f"SAMPLE {i}")
        print('='*60)

        print("ğŸ”¹ PROMPT:")
        prompt = original_df['prompt'].iloc[i]
        if isinstance(prompt, list):
            for msg in prompt:
                print(f"  {msg['role'].upper()}: {msg['content']}")
        else:
            print(prompt)
        
        print("ğŸ”¹ ORIGINAL OUTPUT:")
        print(inference_df['extra_info'].iloc[i].get("output", "None"))
        
        print("\nğŸ”¹ INFERENCE RESPONSE:")
        response = inference_df['responses'].iloc[i]

        # å¤„ç†numpy array
        if isinstance(response, np.ndarray):
            print(response[0])  # å–arrayçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(response, list):
            print(response[0])  # å–listçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        else:
            print(response)
        print("\n" + "-"*80)

def load_specific_id_content(original_path: str, inference_path: str, indexs: list):
    """åŠ è½½å¹¶å±•ç¤ºoutputå’Œresponseå†…å®¹"""
    # åŠ è½½æ•°æ®
    original_df = pd.read_parquet(original_path)
    inference_df = pd.read_parquet(inference_path)
    
    print(f"Original dataset shape: {original_df.shape}")
    print(f"Inference dataset shape: {inference_df.shape}")
    print(f"Original columns: {list(original_df.columns)}")
    print(f"Inference columns: {list(inference_df.columns)}")
    
    for i in indexs:
        print(f"\n{'='*60}")
        print(f"SAMPLE {i}")
        print('='*60)

        print("ğŸ”¹ PROMPT:")
        prompt = original_df['prompt'].iloc[i]
        if isinstance(prompt, list):
            for msg in prompt:
                print(f"  {msg['role'].upper()}: {msg['content']}")
        else:
            print(prompt)
        
        print("ğŸ”¹ ORIGINAL OUTPUT:")
        print(inference_df['extra_info'].iloc[i].get("output", "None"))
        
        print("\nğŸ”¹ INFERENCE RESPONSE:")
        response = inference_df['responses'].iloc[i]

        # å¤„ç†numpy array
        if isinstance(response, np.ndarray):
            print(response[0])  # å–arrayçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(response, list):
            print(response[0])  # å–listçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        else:
            print(response)
        print("\n" + "-"*80)
        

def extract_tools_from_system(system_content: str) -> Dict[str, Dict]:
    """ä»systemå­—æ®µä¸­æå–å·¥å…·å®šä¹‰"""
    # æŸ¥æ‰¾ "Here is a list of functions in JSON format that you can invoke.\n" åé¢çš„éƒ¨åˆ†
    pattern = r"Here is a list of functions in JSON format that you can invoke\.\s*\n(.+?)(?=\n\n|\Z)"
    match = re.search(pattern, system_content, re.DOTALL)
    
    if not match:
        return {}
    
    functions_text = match.group(1).strip()

    functions = json.loads(functions_text)

    return {func['name']: func for func in functions}  # å‡½æ•°åä¸ºkey

def extract_tools_from_instruction(instruction: str) -> Dict[str, Dict]:
    """
    ä»instructionä¸­æå–å·¥å…·å®šä¹‰ä¿¡æ¯
    
    Args:
        instruction: åŒ…å«å·¥å…·å®šä¹‰çš„æŒ‡ä»¤æ–‡æœ¬
        
    Returns:
        Dict[str, Dict]: å·¥å…·åç§°åˆ°å·¥å…·å®šä¹‰çš„æ˜ å°„
    """
    tools_dict = {}
    
    # æå–<tools></tools>æ ‡ç­¾å†…çš„å†…å®¹
    tools_pattern = r'<tools>\n(.*?)\n</tools>'
    tools_match = re.search(tools_pattern, instruction, re.DOTALL)
    
    if not tools_match:
        return tools_dict
    
    tools_content = tools_match.group(1).strip()
    
    # æŒ‰è¡Œåˆ†å‰²ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªJSONå·¥å…·å®šä¹‰
    lines = tools_content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue  
        try:
            tool_def = json.loads(line)
            if 'name' in tool_def:
                tools_dict[tool_def['name']] = tool_def
        except json.JSONDecodeError:
            # è·³è¿‡æ— æ³•è§£æçš„è¡Œ
            continue
    
    return tools_dict

def validate_function_call(call_dict: Dict, tool_set: Dict[str, Dict]) -> Tuple[bool, str]:
    """éªŒè¯å‡½æ•°è°ƒç”¨æ˜¯å¦ç¬¦åˆè§„åˆ™"""
    try:
        func_name = call_dict['name']
        params = call_dict['arguments']
            # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
        if func_name not in tool_set:
            return False, f"Function '{func_name}' not found in tool set"
        
        func_def = tool_set[func_name]
        
        # è·å–å‚æ•°å®šä¹‰
        # if 'parameters' not in func_def['parameters']['properties']:
        #     return False, f"Function '{func_name}' has no the parameter definition of {}"
        
        param_def = func_def['parameters']
        required_params = param_def.get('required', [])
        properties = param_def.get('properties', {})
        
        # æ£€æŸ¥å¿…éœ€å‚æ•°æ˜¯å¦å­˜åœ¨
        for req_param in required_params:
            if req_param not in params:
                return False, f"Required parameter '{req_param}' missing for function '{func_name}'"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢å¤–çš„ä¸åˆè§„å­—æ®µ
        for param_name in params:
            if param_name not in properties:
                return False, f"Invalid parameter '{param_name}' for function '{func_name}'"
        
        # æ£€æŸ¥å‚æ•°ç±»å‹
        for param_name, param_value in params.items():
            if param_name in properties:
                expected_type = properties[param_name].get('type')
                if expected_type:
                    if not validate_parameter_type(param_value, expected_type):
                        return False, f"Parameter '{param_name}' has invalid type for function '{func_name}'. Expected {expected_type}, got {type(param_value).__name__}"
    
        return True, ""
    except Exception as e:
        return False, f"Validation error: {str(e)}"

def validate_parameter_type(value: Any, expected_type: str) -> bool:
    """éªŒè¯å‚æ•°ç±»å‹"""
    type_mapping = {
        'string': str,
        'integer': int,
        'number': (int, float),
        'boolean': bool,
        'array': list,
        'object': dict
    }
    
    expected_python_type = type_mapping.get(expected_type.lower())
    if expected_python_type is None:
        return True  # æœªçŸ¥ç±»å‹ï¼Œè·³è¿‡æ£€æŸ¥
    
    return isinstance(value, expected_python_type)

def create_llm_judge_prompt(original_row: Dict, output: str, tool_set: Dict, date: str) -> str:
    """åˆ›å»ºé€‚é…Qwen3æ¨¡å‹çš„åˆ¤æ–­promptï¼Œè¿”å›èŠå¤©æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨"""
    
    conversation_text = "<|im_start|>user\n" + original_row['input'] + "<|im_end|>\n"
    tool_str = ""
    for tool in tool_set.values():
        tool_str += f"\n{json.dumps(tool)}"

    # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯ - é’ˆå¯¹Qwen3ä¼˜åŒ–
    system_message = {
        "role": "system",
        "content": f"""You are a strict evaluator for tool call correctness in dialogues. Please evaluate whether the assistant's tool invocation in this turn is appropriate, based on the provided user query, the definition of the toolset, and the dialogue context. 
Here are the available tools in the conversation:
<tools>{tool_str}\n</tools>

The date is {date}.

Evaluation Criteria:
1. Correctness: Whether the function calls properly address the user's request
2. Parameter Accuracy: Whether all parameters are correct and appropriate  
3. Function Selection: Whether the chosen functions are suitable for the task
4. Completeness: Whether the response fully satisfies the user's needs

Please provide objective and thorough evaluations based on these criteria."""
    }
    
    # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯ - é’ˆå¯¹Qwen3ä¼˜åŒ–æ ¼å¼
    user_message = {
        "role": "user", 
        "content": f"""## Task
Please evaluate the last function call responses for the following conversation:

**Original Conversation:**
{conversation_text}

**Responses to Evaluate:**
{output}

## Output Format
Strictly respond with the following formats (no additional text):
"[INCORRECT/CORRECT].\nError Analysis: [When the resposne is judged to be INCORRECT, an analysis is provided - max 2 sentences] "
"""
    }
    
    return [system_message, user_message]

def remove_reasoning_content(model_response):
    if "</think>" in model_response:
        parts = model_response.split("</think>")
        reasoning_content = parts[0].rstrip("\n").split("<think>")[-1].lstrip("\n")
        cleaned_response = parts[-1].strip("\n")
        return cleaned_response
    else:
        return model_response

def _extract_tool_calls(input_string):
    pattern = r"<tool_call>\n(.*?)\n</tool_call>"
    matches = re.findall(pattern, input_string, re.DOTALL)

    # Process matches into a list of dictionaries
    result = []
    for match in matches:
        try:
            match = json.loads(match)
        except Exception as e:
            pass
        result.append(match)
    return result

def call_llm_judge(messages: List[Dict], client: OpenAI, model: str = "Qwen3-32b") -> str:
    """è°ƒç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œåˆ¤æ–­"""
    try:
        response = client.chat.completions.create(
            model=model,
            temperature=0,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
            max_tokens=2048,   # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ç®€çŸ­åˆ¤æ–­
            messages=messages,
            timeout=72000,
            top_p=0.6,
            presence_penalty=1.5,
            extra_body={
                "top_k": 20,
                "chat_template_kwargs": {"enable_thinking": True}
            })

        response_content = response.choices[0].message.content.strip().strip("\n")
        return remove_reasoning_content(response_content)
    except Exception as e:
        print(f"API_ERROR: {str(e)}")
        return f"API_ERROR: {str(e)}"

def call_llm_judge_with_consensus(
    messages: List[Dict], 
    client_manager: ClientManager, 
    models: List[str]
) -> Tuple[str, bool, Dict]:
    """
    ä½¿ç”¨å¤šä¸ªLLMè¿›è¡Œåˆ¤æ–­ï¼Œè¿”å›å¤šæ•°æŠ•ç¥¨ç»“æœ
    
    Returns:
        Tuple[str, bool, Dict]: (æœ€ç»ˆåˆ¤æ–­ç»“æœ, æ˜¯å¦è¾¾æˆä¸€è‡´, è¯¦ç»†ä¿¡æ¯)
    """
    
    results = []
    full_responses = []
    
    # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œåˆ¤æ–­
    for model in models:
        try:
            client = client_manager.get_random_client()
            
            result = call_llm_judge(messages, client, model)
            full_responses.append(result)
            
            # æå–æ ¸å¿ƒåˆ¤æ–­ç»“æœ
            if 'INCORRECT' in result:
                core_result = 'INCORRECT'
            elif 'CORRECT' in result:  
                core_result = 'CORRECT'
            else:
                core_result = 'UNKNOWN'
            
            results.append(core_result)
            
        except Exception as e:
            results.append('ERROR')
            full_responses.append(f"API_ERROR: {str(e)}")
    
    # å¤šæ•°æŠ•ç¥¨
    result_counter = Counter(results)
    most_common_result, most_common_count = result_counter.most_common(1)[0]
    
    # åˆ¤æ–­æ˜¯å¦è¾¾æˆä¸€è‡´ï¼ˆè‡³å°‘2/3åŒæ„ï¼‰
    consensus_achieved = most_common_count >= 2
    
    # æ‰¾åˆ°å¯¹åº”çš„å®Œæ•´å›å¤
    final_response = "No valid response"
    for i, core_result in enumerate(results):
        if core_result == most_common_result:
            final_response = full_responses[i]
            break
    
    # æ„å»ºè¯¦ç»†ä¿¡æ¯
    consensus_info = {
        'vote_results': results,
        'vote_distribution': dict(result_counter),
        'consensus_achieved': consensus_achieved,
        'final_result': most_common_result,
        'vote_count': f"{most_common_count}/{len(models)}",
        'all_responses': full_responses
    }
    
    return final_response, consensus_achieved, consensus_info
    
def extract_error_message(judge_result, result):
    if result['status'] == 'llm_judge_failed':
        return f"Unexpected judge result: {judge_result}"
    else:
        start_pos = judge_result.find("Error Analysis")
        if start_pos != -1:
            return judge_result[start_pos:]

def extract_date_from_instruction(text):
    patterns = [
        r'Today is (\d{4}-\d{2}-\d{2})', # YYYY-MM-DD
        r'Today is (\d{2}/\d{2}/\d{4})', # MM/DD/YYYY
        r'Today is (\d{2}-\d{2}-\d{4})', # MM-DD-YYYY
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    return None

def evaluate_single_sample(index: int, original_row: dict, client_manager: ClientManager, models: List[str], progress_lock: threading.Lock, processed_count: list) -> Tuple[int, Dict[str, Any]]:
    """è¯„ä¼°å•ä¸ªæ ·æœ¬ï¼Œè¿”å›indexå’Œç»“æœ - ä½¿ç”¨å¤šæ¨¡å‹consensus"""
    
    result = copy.deepcopy(original_row)
    result['status'] = ""
    result['error_message'] = ""
    result['consensus_info'] = {}

    output = original_row['output']
        
    # æ£€æŸ¥åŸå§‹outputæ˜¯å¦ä»¥"<tool_call>"å¼€å¤´
    if not str(output).strip().startswith("<tool_call>"):
        result['status'] = 'not_function_call'
        with progress_lock:
            processed_count[0] += 1
            if processed_count[0] % 10 == 0:
                print(f"Processed {processed_count[0]} samples...")
        return index, result
        
    # å°è¯•è§£æåŸå§‹output
    original_calls = _extract_tool_calls(output)
    
    tool_set = extract_tools_from_instruction(original_row['instruction'])
    date = extract_date_from_instruction(original_row['instruction'])

    # éªŒè¯æ¨¡å‹è°ƒç”¨æ˜¯å¦ç¬¦åˆè§„åˆ™
    for call in original_calls:
        is_valid, violation_msg = validate_function_call(call, tool_set)

        if not is_valid:
            result['status'] = 'rule_violation'
            result['error_message'] += violation_msg

    if result['status'] == 'rule_violation':
        with progress_lock:
            processed_count[0] += 1
            if processed_count[0] % 10 == 0:
                print(f"Processed {processed_count[0]} samples...")
        return index, result
                
    # ä½¿ç”¨å¤šæ¨¡å‹consensusè¿›è¡Œåˆ¤æ–­
    judge_messages = create_llm_judge_prompt(original_row, output, tool_set, date)
    judge_result, consensus_achieved, consensus_info = call_llm_judge_with_consensus(
        judge_messages, client_manager, models
    )
    
    # ä¿å­˜consensusä¿¡æ¯
    result['consensus_info'] = consensus_info
    
    # æ ¹æ®æœ€ç»ˆåˆ¤æ–­ç»“æœè®¾ç½®çŠ¶æ€
    final_result = consensus_info['final_result']
    if final_result == 'INCORRECT':
        result['status'] = 'incorrect'
        result['error_message'] = extract_error_message(judge_result, result)
    elif final_result == 'CORRECT':
        result['status'] = 'correct'
    else:
        result['status'] = 'llm_judge_failed'
        result['error_message'] = f"Unexpected judge result: {final_result}"
    
    # å¦‚æœæ²¡æœ‰è¾¾æˆconsensusï¼Œæ ‡è®°çŠ¶æ€
    if not consensus_achieved:
        result['status'] += '_no_consensus'

    # æ›´æ–°è¿›åº¦
    with progress_lock:
        processed_count[0] += 1
        if processed_count[0] % 10 == 0:
            print(f"Processed {processed_count[0]} samples...")
    
    return index, result

def comprehensive_evaluation(json_path: str, api_keys: List[str], base_url: str, models: List[str], max_workers: int = 8) -> Dict[str, Any]:
    """å¯¹è¯¥æ¬¡æ¨ç†è¿›è¡Œç»¼åˆè¯„ä¼° - å¤šæ¨¡å‹consensusç‰ˆæœ¬"""
    
    # åˆ›å»ºå®¢æˆ·ç«¯ç®¡ç†å™¨
    client_manager = ClientManager(api_keys, base_url)
    print(f"Created client manager with {len(client_manager.clients)} API clients")
    print(f"Using models: {models}")
    
    with open(json_path) as f:
        original_data = json.load(f)
    
    # åˆ›å»ºçº¿ç¨‹é”å’Œè¿›åº¦è®¡æ•°å™¨
    progress_lock = threading.Lock()
    processed_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨å‡½æ•°é—´å…±äº«
    
    # å‡†å¤‡ç»“æœåˆ—è¡¨
    results = [None] * len(original_data)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_index = {
            executor.submit(evaluate_single_sample, i, original_data[i], client_manager, models, progress_lock, processed_count): i 
            for i in range(len(original_data))
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_index):
            try:
                index, result = future.result()
                results[index] = result
            except Exception as exc:
                original_index = future_to_index[future]
                print(f'Sample {original_index} generated an exception: {exc}')
                # åˆ›å»ºé”™è¯¯ç»“æœ
                error_result = copy.deepcopy(original_data[original_index])
                error_result['status'] = 'processing_error'
                error_result['error_message'] = f'Processing exception: {str(exc)}'
                error_result['consensus_info'] = {}
                results[original_index] = error_result
    
    # ç»Ÿè®¡ç»“æœ
    status_counts = Counter([r['status'] for r in results])
    
    # ç»Ÿè®¡consensusä¿¡æ¯
    consensus_stats = {
        'total_consensus': 0,
        'no_consensus': 0,
        'vote_distribution': Counter()
    }
    
    for result in results:
        if 'consensus_info' in result and result['consensus_info']:
            if result['consensus_info'].get('consensus_achieved', False):
                consensus_stats['total_consensus'] += 1
            else:
                consensus_stats['no_consensus'] += 1
            
            # ç»Ÿè®¡æŠ•ç¥¨åˆ†å¸ƒ
            vote_count = result['consensus_info'].get('vote_count', 'unknown')
            consensus_stats['vote_distribution'][vote_count] += 1
    
    evaluation_summary = {
        'total_samples': len(results),
        'status_distribution': dict(status_counts),
        'consensus_statistics': consensus_stats,
        'models_used': models
    }
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "="*60)
    print("EVALUATION SUMMARY")
    print("="*60)
    print(f"Total samples: {evaluation_summary['total_samples']}")
    print(f"Used {len(client_manager.clients)} API clients with {max_workers} threads")
    print(f"Models used: {', '.join(models)}")
    print("\nStatus distribution:")
    for status, count in status_counts.items():
        percentage = (count / len(results)) * 100
        print(f"  {status}: {count} ({percentage:.1f}%)")
    
    print(f"\nConsensus Statistics:")
    print(f"  Achieved consensus: {consensus_stats['total_consensus']}")
    print(f"  No consensus: {consensus_stats['no_consensus']}")
    print(f"  Vote distribution: {dict(consensus_stats['vote_distribution'])}")
    
    return results

def main():
    json_path = "xxx.json"


    models = ["Qwen3-32B"]  # æ ¹æ®å®é™…å¯ç”¨æ¨¡å‹è°ƒæ•´
    # å¤šä¸ªAPI key
    api_keys = []
    
    base_url = "xxx"
    
    max_workers = min(12, 2*len(api_keys))  # é™ä½å¹¶å‘æ•°ä»¥é¿å…APIé™æµ
    
    # è¿›è¡Œç»¼åˆè¯„ä¼°
    print(f"\nStarting comprehensive evaluation with consensus approach...")
    print(f"Using {len(api_keys)} API keys with {max_workers} threads")
    
    evaluation_results = comprehensive_evaluation(
        json_path, 
        api_keys, 
        base_url, 
        models,
        max_workers
    )
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    output_path = "xxx.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
    print(f"\nEvaluation results saved to: {output_path}")

if __name__ == "__main__":
    main()
